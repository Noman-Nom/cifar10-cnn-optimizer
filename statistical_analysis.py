"""
Comprehensive statistical analysis of hyperparameter optimization results.
"""
import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
from scipy.stats import spearmanr, pearsonr
import warnings
warnings.filterwarnings('ignore')


def load_all_data(results_dir):
    """Load all experiment data."""
    results_dir = Path(results_dir)
    
    # Load histories
    rs_path = results_dir / 'random_search' / 'run_42' / 'optimization_history.json'
    pso_path = results_dir / 'pso' / 'run_42' / 'optimization_history.json'
    summary_path = results_dir / 'summary.json'
    
    with open(rs_path, 'r') as f:
        rs_history = json.load(f)
    with open(pso_path, 'r') as f:
        pso_history = json.load(f)
    with open(summary_path, 'r') as f:
        summary = json.load(f)
    
    return rs_history, pso_history, summary


def analyze_hyperparameter_correlations(rs_history, pso_history):
    """Compute correlations between hyperparameters and performance."""
    
    # Combine all data
    all_configs = []
    all_scores = []
    all_methods = []
    
    for h in rs_history:
        all_configs.append(h['config'])
        all_scores.append(h['score'])
        all_methods.append('Random Search')
    
    for h in pso_history:
        all_configs.append(h['config'])
        all_scores.append(h['score'])
        all_methods.append('PSO')
    
    df = pd.DataFrame(all_configs)
    df['score'] = all_scores
    df['method'] = all_methods
    
    # Calculate correlations
    correlations = []
    for col in df.columns:
        if col not in ['score', 'method']:
            # Pearson correlation
            pearson_r, pearson_p = pearsonr(df[col], df['score'])
            # Spearman correlation
            spearman_r, spearman_p = spearmanr(df[col], df['score'])
            
            correlations.append({
                'hyperparameter': col,
                'pearson_r': pearson_r,
                'pearson_p': pearson_p,
                'spearman_r': spearman_r,
                'spearman_p': spearman_p,
                'abs_pearson': abs(pearson_r),
                'significant': pearson_p < 0.05
            })
    
    corr_df = pd.DataFrame(correlations)
    corr_df = corr_df.sort_values('abs_pearson', ascending=False)
    
    return corr_df, df


def analyze_optimizer_comparison(summary):
    """Compare optimizer performance statistically."""
    
    rs_accs = summary['random_search']['test_accuracies']
    pso_accs = summary['pso']['test_accuracies']
    
    results = {
        'Random Search': {
            'mean': np.mean(rs_accs),
            'std': np.std(rs_accs),
            'min': np.min(rs_accs),
            'max': np.max(rs_accs),
            'median': np.median(rs_accs),
            'n': len(rs_accs)
        },
        'PSO': {
            'mean': np.mean(pso_accs),
            'std': np.std(pso_accs),
            'min': np.min(pso_accs),
            'max': np.max(pso_accs),
            'median': np.median(pso_accs),
            'n': len(pso_accs)
        }
    }
    
    # Statistical tests (if multiple runs)
    tests = {}
    if len(rs_accs) >= 2 and len(pso_accs) >= 2:
        # Mann-Whitney U test
        u_stat, u_p = stats.mannwhitneyu(rs_accs, pso_accs, alternative='two-sided')
        tests['mann_whitney'] = {'statistic': u_stat, 'p_value': u_p}
        
        # T-test
        t_stat, t_p = stats.ttest_ind(rs_accs, pso_accs)
        tests['t_test'] = {'statistic': t_stat, 'p_value': t_p}
    
    if len(rs_accs) >= 3 and len(pso_accs) >= 3 and len(rs_accs) == len(pso_accs):
        # Wilcoxon signed-rank test
        w_stat, w_p = stats.wilcoxon(rs_accs, pso_accs)
        tests['wilcoxon'] = {'statistic': w_stat, 'p_value': w_p}
    
    return results, tests


def analyze_search_efficiency(rs_history, pso_history):
    """Analyze search efficiency metrics."""
    
    # Random Search efficiency
    rs_scores = [h['score'] for h in rs_history]
    rs_best_found_at = np.argmax(rs_scores) + 1
    rs_cummax = np.maximum.accumulate(rs_scores)
    
    # PSO efficiency
    pso_scores = [h['score'] for h in pso_history]
    pso_best_found_at = np.argmax(pso_scores) + 1
    pso_cummax = np.maximum.accumulate(pso_scores)
    
    # Calculate area under convergence curve (higher is better)
    rs_auc = np.trapz(rs_cummax) / len(rs_cummax)
    pso_auc = np.trapz(pso_cummax) / len(pso_cummax)
    
    efficiency = {
        'Random Search': {
            'total_evaluations': len(rs_scores),
            'best_score': max(rs_scores),
            'best_found_at_eval': rs_best_found_at,
            'efficiency_ratio': rs_best_found_at / len(rs_scores),
            'mean_score': np.mean(rs_scores),
            'std_score': np.std(rs_scores),
            'convergence_auc': rs_auc
        },
        'PSO': {
            'total_evaluations': len(pso_scores),
            'best_score': max(pso_scores),
            'best_found_at_eval': pso_best_found_at,
            'efficiency_ratio': pso_best_found_at / len(pso_scores),
            'mean_score': np.mean(pso_scores),
            'std_score': np.std(pso_scores),
            'convergence_auc': pso_auc
        }
    }
    
    return efficiency


def analyze_best_configurations(rs_history, pso_history):
    """Analyze the best configurations found."""
    
    rs_best = max(rs_history, key=lambda x: x['score'])
    pso_best = max(pso_history, key=lambda x: x['score'])
    
    # Compare configurations
    comparison = []
    for key in rs_best['config'].keys():
        comparison.append({
            'hyperparameter': key,
            'random_search': rs_best['config'][key],
            'pso': pso_best['config'][key],
            'difference': abs(rs_best['config'][key] - pso_best['config'][key])
        })
    
    comparison_df = pd.DataFrame(comparison)
    
    return rs_best, pso_best, comparison_df


def main():
    # Results directory
    results_dir = Path('experiments/experiments/results/CIFAR10_CNN_20260127_170821')
    
    if not results_dir.exists():
        print(f"‚ùå Results directory not found: {results_dir}")
        return
    
    # Create output directory
    output_dir = results_dir / 'statistical_analysis'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*70)
    print("COMPREHENSIVE STATISTICAL ANALYSIS")
    print("="*70)
    
    # Load data
    rs_history, pso_history, summary = load_all_data(results_dir)
    
    # 1. Hyperparameter Correlations
    print("\n" + "="*70)
    print("1. HYPERPARAMETER CORRELATIONS WITH PERFORMANCE")
    print("="*70)
    
    corr_df, full_df = analyze_hyperparameter_correlations(rs_history, pso_history)
    print("\nCorrelation Analysis (sorted by importance):")
    print("-"*70)
    print(corr_df.to_string(index=False))
    
    corr_df.to_csv(output_dir / 'hyperparameter_correlations.csv', index=False)
    
    # 2. Optimizer Comparison
    print("\n" + "="*70)
    print("2. OPTIMIZER PERFORMANCE COMPARISON")
    print("="*70)
    
    perf_results, stat_tests = analyze_optimizer_comparison(summary)
    
    print("\nPerformance Summary:")
    print("-"*70)
    perf_df = pd.DataFrame(perf_results).T
    print(perf_df.to_string())
    
    perf_df.to_csv(output_dir / 'optimizer_comparison.csv')
    
    if stat_tests:
        print("\nStatistical Tests:")
        print("-"*70)
        for test_name, test_result in stat_tests.items():
            sig = "‚úÖ Significant" if test_result['p_value'] < 0.05 else "‚ùå Not Significant"
            print(f"{test_name}: statistic={test_result['statistic']:.4f}, p-value={test_result['p_value']:.4f} {sig}")
    
    # 3. Search Efficiency
    print("\n" + "="*70)
    print("3. SEARCH EFFICIENCY ANALYSIS")
    print("="*70)
    
    efficiency = analyze_search_efficiency(rs_history, pso_history)
    
    print("\nEfficiency Metrics:")
    print("-"*70)
    eff_df = pd.DataFrame(efficiency).T
    print(eff_df.to_string())
    
    eff_df.to_csv(output_dir / 'search_efficiency.csv')
    
    # 4. Best Configurations
    print("\n" + "="*70)
    print("4. BEST CONFIGURATIONS COMPARISON")
    print("="*70)
    
    rs_best, pso_best, config_comparison = analyze_best_configurations(rs_history, pso_history)
    
    print(f"\nRandom Search Best (Score: {rs_best['score']:.2f}%):")
    print("-"*70)
    for k, v in rs_best['config'].items():
        print(f"  {k}: {v}")
    
    print(f"\nPSO Best (Score: {pso_best['score']:.2f}%):")
    print("-"*70)
    for k, v in pso_best['config'].items():
        print(f"  {k}: {v}")
    
    config_comparison.to_csv(output_dir / 'best_configs_comparison.csv', index=False)
    
    # 5. Key Insights
    print("\n" + "="*70)
    print("5. KEY INSIGHTS")
    print("="*70)
    
    # Most important hyperparameters
    top_hp = corr_df.head(3)['hyperparameter'].tolist()
    print(f"\nüìä Most Important Hyperparameters: {', '.join(top_hp)}")
    
    # Winner
    rs_final = summary['random_search']['test_accuracies'][0]
    pso_final = summary['pso']['test_accuracies'][0]
    winner = "Random Search" if rs_final > pso_final else "PSO"
    print(f"\nüèÜ Best Method: {winner} ({max(rs_final, pso_final):.2f}% test accuracy)")
    
    # Optimal ranges
    print("\nüìà Optimal Hyperparameter Patterns (from best configs):")
    print(f"   - Learning Rate: ~0.0004 - 0.0007")
    print(f"   - Batch Size: Small (32-53)")
    print(f"   - Conv Channels: High (82-116)")
    print(f"   - Num Conv Blocks: Maximum (4)")
    print(f"   - FC Hidden: Large (370-512)")
    print(f"   - Dropout: Moderate (0.33-0.37)")
    print(f"   - Weight Decay: Low (~0.0001)")
    
    # 6. Save full data
    full_df.to_csv(output_dir / 'all_evaluations.csv', index=False)
    
    # Summary report
    report = f"""
HYPERPARAMETER OPTIMIZATION EXPERIMENT REPORT
=============================================

Date: 2026-01-27
Dataset: CIFAR-10
Model: ModernCNN

RESULTS SUMMARY
---------------
Random Search:
  - Exploration Score: {rs_best['score']:.2f}%
  - Final Test Accuracy: {rs_final:.2f}%
  - Total Trials: {len(rs_history)}

PSO:
  - Exploration Score: {pso_best['score']:.2f}%
  - Final Test Accuracy: {pso_final:.2f}%
  - Total Evaluations: {len(pso_history)}

WINNER: {winner} (by {abs(rs_final - pso_final):.2f}%)

TOP 3 IMPORTANT HYPERPARAMETERS:
{chr(10).join([f"  {i+1}. {hp}" for i, hp in enumerate(top_hp)])}

RECOMMENDATIONS:
- Use num_conv_blocks=4 (maximum depth)
- Keep batch_size small (32-64)
- Use moderate dropout (0.3-0.4)
- Learning rate around 0.0005
"""
    
    with open(output_dir / 'experiment_report.txt', 'w') as f:
        f.write(report)
    
    print("\n" + "="*70)
    print(f"‚úÖ All analysis saved to: {output_dir}")
    print("="*70)
    print("\nFiles generated:")
    print("  - hyperparameter_correlations.csv")
    print("  - optimizer_comparison.csv")
    print("  - search_efficiency.csv")
    print("  - best_configs_comparison.csv")
    print("  - all_evaluations.csv")
    print("  - experiment_report.txt")


if __name__ == "__main__":
    main()